import streamlit as st
import yfinance as yf
import feedparser
import pandas as pd
import matplotlib.pyplot as plt
import japanize_matplotlib
from transformers import pipeline
from sklearn.linear_model import LinearRegression
import urllib.parse
import numpy as np

# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="AIæŠ•è³‡è¨ºæ–­ã‚¢ãƒ—ãƒª", layout="wide")
st.title("ğŸš€ AIéŠ˜æŸ„è¨ºæ–­ãƒ©ãƒ³ã‚­ãƒ³ã‚°")

# --- 2. AIãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ ---
@st.cache_resource
def load_ai():
    return pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

analyzer = load_ai()

# --- 3. éŠ˜æŸ„è¨­å®š ---
stocks = {
    "ãƒ†ã‚¹ãƒ©": "TSLA",
    "ãƒ‘ãƒ©ãƒ³ãƒ†ã‚£ã‚¢": "PLTR",
    "ãƒˆãƒ¨ã‚¿": "7203.T",
    "ä»»å¤©å ‚": "7974.T",
    "ã‚¨ãƒŒãƒ“ãƒ‡ã‚£ã‚¢": "NVDA",
    "Apple": "AAPL"
}
selected_names = st.sidebar.multiselect("åˆ†æã™ã‚‹éŠ˜æŸ„ã‚’é¸æŠ", list(stocks.keys()), default=["ãƒ†ã‚¹ãƒ©", "ãƒ‘ãƒ©ãƒ³ãƒ†ã‚£ã‚¢", "ãƒˆãƒ¨ã‚¿"])

# --- 4. å®Ÿè¡Œãƒœã‚¿ãƒ³ ---
if st.sidebar.button("åˆ†æã‚’å®Ÿè¡Œ"):
    results = []
    
    with st.spinner('AIãŒåˆ†æä¸­...'):
        for name in selected_names:
            try:
                symbol = stocks[name]
                df = yf.download(symbol, period="3mo", progress=False)
                if len(df) < 10: continue

                # äºˆæ¸¬è¨ˆç®—ï¼ˆã‚¨ãƒ©ãƒ¼ãŒå‡ºã«ãã„æ›¸ãæ–¹ã«ä¿®æ­£ï¼‰
                df_study = df.tail(30).copy()
                y = df_study['Close'].values.reshape(-1, 1)
                X = np.arange(len(y)).reshape(-1, 1)
                
                model = LinearRegression()
                model.fit(X, y)
                pred_price = model.predict([[len(y)]])[0][0]
                
                last_price = float(y[-1])
                diff_pct = ((pred_price - last_price) / last_price) * 100
                
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æ
                query = urllib.parse.quote(name)
                url = f"https://news.google.com/rss/search?q={query}&hl=ja&gl=JP&ceid=JP:ja"
                feed = feedparser.parse(url)
                
                stars, count = 0, 0
                for entry in feed.entries[:2]:
                    res = analyzer(entry.title)[0]
                    stars += int(res['label'].split()[0])
                    count += 1
                avg_stars = stars / count if count > 0 else 3
                
                results.append({
                    "éŠ˜æŸ„": name,
                    "ç¾åœ¨ä¾¡æ ¼": round(last_price, 2),
                    "AIäºˆæ¸¬(æ˜æ—¥)": round(float(pred_price), 2),
                    "æœŸå¾…å€¤(%)": round(float(diff_pct), 2),
                    "ãƒ‹ãƒ¥ãƒ¼ã‚¹è©•ä¾¡": f"{avg_stars:.1f} â˜…",
                    "score": float(diff_pct) + (avg_stars - 3)
                })
            except Exception as e:
                st.warning(f"{name}ã®åˆ†æä¸­ã«å°ã•ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼‰")
                continue

    if results:
        res_df = pd.DataFrame(results).sort_values(by="score", ascending=False)
        res_df.insert(0, "é †ä½", range(1, len(res_df) + 1))
        st.subheader("ğŸ† æ³¨ç›®éŠ˜æŸ„ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
        st.table(res_df.drop(columns="score"))
    else:
        st.error("ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
